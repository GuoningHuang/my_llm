1 Introduction  Deep Neural Network (DNN) applications achieve orders of magnitude higher throughput-per-Watt when executing on GPUs compared to CPUs [89, 96, 99, 101]. However, due to the high price of GPU hardware, the ultimate energy and cost savings of using GPUs for DNN jobs depend on operating the accelerators at high utilization. Despite the high compute and memory intensity of DNN computations [86], individual DNN workloads often underutilize GPU hardware [98, 99]. Latency-critical inference jobs, such as Autonomous Driving [77], fraud detection and recommendation systems [16] use small batch sizes to meet service level objectives, resulting in insufficient parallelism to keep GPU compute units busy [49, 50, 75]. Training jobs attempt to maximize batch size for high throughput, however the batch size affects model convergence and must be tuned with other hyperparameters [47, 56, 88, 95, 97]. Hence, in practice, training jobs run with “large enough” batch sizes, which may underutilize sizeable GPU memory capacity. Training jobs may also stall waiting for input data [48, 62, 72, 73, 108] or bottleneck on communication [80, 107], leaving GPUs idle.However, even if we eliminate input data stalls (e.g., by scaling out data preprocessing [34, 48, 108]), alleviate communication stalls (e.g., with asynchronous updates [40, 44], gradient compression [68], and in-network aggregation [85]), and pragmatically maximize batch sizes, DNN workloads still underutilize GPU hardware. Figure 1 shows the fundamental reason why: a DNN workload consists of many datadependent operators that run for short periods of time (10s1000s of μs), each with different compute and memory requirements. Utilization is bursty and low on average (see red dotted lines) as individual operators saturate compute units or memory bandwidth, but often leave a resource type idle. For the MobileNetV2 training job in Figure 1, GPU compute throughput and memory bandwidth utilization are below 40% and 55%, on average, respectively. The problem of GPU underutilization is only getting worse as hardware vendors continue to scale GPU memory and compute capacity [11]. A common solution is to share GPUs between jobs. The main challenge is maximizing utilization while mitigating interference between jobs for high performance. Temporal sharing techniques time-slice the GPU at the granularity of an inference request or training minibatch [39, 49, 100, 102104]. This can lead to head-of-line blocking, since incoming inference requests or training minibatches need to wait for the ongoing tasks to finish execution on the GPU before being scheduled (see section 6), and still wastes resources when the operators of individual tasks do not consume all GPU compute or memory bandwidth. Spatial sharing improves utilization, however, current techniques are either too coarse-grained (e.g., Multi-Instance GPUs [12], Zico [67], Tick-Tock [94]) or are not sufficiently interference-aware (e.g., Multi-Process Service (MPS) [25], GPU Streams [15], REEF [50], Paella [75]). Figure 2 shows that state-of-the-art GPU sharing techniques leave performance on the table. We collocate three pairs of DNN jobs whose aggregate resource requirements fit on a single V100-16GB GPU (see Table 1). The first job in each pair is high-priority while the second is best-effort. Each job issues one request at a time in a closed loop. The stacked bar plot shows each job’s throughput on a shared GPU with various techniques, compared to the total throughput when jobs execute on dedicated GPUs (denoted as Ideal). Temporal sharing, MPS, Streams, and Tick-Tock (proposed for training job collocation) achieve far below ideal aggregate throughput. REEF achieves high performance for the high-priority job, but barely executes the best-effort job. To help close this gap, we propose Orion1, a fine-grained, interference-aware GPU scheduler. Orion maintains performance for a high-priority workload while collocating besteffort jobs to maximize GPU utilization and save costs. Orion is a software system that intercepts GPU kernel launches  1Orion is available at https://github.com/eth-easl/orion  Figure 2. Existing GPU collocation techniques leave performance on the table. Bold bars show high-priority DNN job throughput, faint bars show best-effort job throughput.  from client applications sharing a GPU device. Orion schedules requests based on client job priority, operator size, and whether an operator is compute or memory bound. By scheduling at the granularity of individual operations, Orion spatially shares the GPU to make best use of GPU compute units and memory bandwidth that may be underutilized by the high-priority job for only 10-1000s of μs. Orion improves GPU resource efficiency (and cost) for a variety of DNN collocation use cases, with minimal impact on high-priority job performance. When collocating latencysensitive inference with best-effort offline inference, Orion improves aggregate throughput by up to 7.3× compared to dedicated GPU execution, while maintaining p99 latency within 15% on average for the high-priority job. When collocating a latency-sensitive inference job with a training job, Orion maintains p99 inference latency within 14% on average while increasing the GPU’s aggregate throughput up to 2.3×. Orion reduces cost by 1.29× when collocating training jobs while ensuring the high-priority training job maintains throughput within 16% of its dedicated GPU throughput.  2 GPU Architecture Background  Figure 3 shows a typical GPU architecture. Without loss of generality, we use NVIDIA hardware and CUDA programming terminology [2, 13]. A GPU consists of multiple Streaming Multiprocessors (SMs), each containing various types of compute cores (e.g., fp64 units, tensor cores), register files, and L1 cache. The GPU also has shared caches and memory. GPU programming abstractions. Developers define their DNN application as a collection of operations using highlevel APIs in a framework like PyTorch [79] or TensorFlow [29]. The application framework compiles these operations (e.g., convolution, batch normalization) for the target GPU architecture and submits operations as CUDA computation kernels to the GPU, along with CUDA memory management operations that allocate, initialize, and free GPU memory.Submitting a kernel involves specifying its resource requirements (e.g., the number of thread blocks2, registers, threads per block, and shared memory required). The application associates each kernel launch and memory operation to a particular CUDA stream. A stream is a sequence of operations that are guaranteed to execute in order. Each application process has its own default stream. To increase concurrency, applications can create additional streams (optionally with different priorities [15]) and submit kernels across streams.  GPU hardware scheduling. As shown in Figure 3, the GPU buffers each CUDA stream’s kernels in a separate work queue on the device. Most GPUs, in particular NVIDIA GPUs, do not allow users to preempt kernels after submission [50]. The GPU hardware scheduler dispatches thread blocks from kernels in each work queue based on stream priority. The scheduler assigns a thread block to an SM when the thread block’s data dependencies are met and an SM with sufficient resources is available. Users cannot control which SM will execute a particular thread block, though researchers have reverse-engineered hardware scheduling policies for popular GPU architectures [30, 45, 46]. When a thread block is assigned to an SM, the SM will schedule and execute all thread warps from that block. SMs can execute multiple warps concurrently, from thread blocks that may belong to different kernels and streams [2]. However, if any warp saturates a resource on the SM (e.g. the number of registers), the SM’s warp scheduler will wait until no resource is saturated before scheduling any additional warps, even if some resources on the SM are available (e.g. compute units or shared memory). GPU utilization metrics. The most common GPU utilization metric is SM utilization, which is the percentage of SMs that are busy (i.e., executing at least one warp). SM utilization does not fully capture GPU utilization as an SM is considered busy even if only a small part of its resources are in use. Compute throughput utilization is the utilization of SM compute units, such as FP32, FP64, FP16, FMA units, tensor cores, etc. [9]. Using the NVIDIA Nsight Compute tool [26], we can get information about the utilization of each individual component 3. The reported utilization is the maximum of all distinct components utilizations. Memory capacity utilization is the percentage of memory allocated on the GPU. Memory bandwidth utilization is the percentage of peak GPU internal memory bandwidth consumed.  3 Understanding DNN GPU Utilization  Although DNN applications typically have high compute and memory intensity [86], they often underutilize GPUs [98, 99, 102, 103]. Prior work has identified reasons for low GPU utilization and proposed solutions. Input data preprocessing bottlenecks on host CPUs can leave GPUs idle while waiting to ingest data [58, 62, 72, 73]. We can alleviate input stalls by disaggregating and scaling out data preprocessing [34, 48, 91, 108]. Communication between nodes can limit distributed training throughput and idle GPUs [80, 107]. Aggressive pipelining [53, 74], gradient compression [68], asynchronous updates [40, 44], in-network aggregation [85] help hide communication stalls. Gang scheduling in multiGPU clusters can leave some GPUs idle while other GPUs for a job become available [103]. Recent DNN systems address this issue with elastic GPU allocation [33, 66, 76, 82]. However, even after eliminating input data, communication, and gang-scheduling bottlenecks, DNN jobs still struggle to keep GPUs fully utilized, especially when modest batch sizes are used. Real-time inference jobs, such as computervision tasks in self-driving cars [42, 77], speech recognition services [42] and online recommendation systems [16] usually employ small batch sizes, in order to avoid SLO violations [41, 49, 50, 75]. Throughput-oriented training jobs use large batch sizes, but maximizing batch sizes to reach GPU memory limits is not always beneficial [71, 78, 95]. Increasing the batch size beyond a certain point can degrade the statistical efficiency of training [56, 57, 61, 70, 82, 88], and have diminishing returns in the training procedure, increasing the time needed to reach a target accuracy [78], and decreasing the model’s validation performance [82]. Shallue et al. [88] studied the effects of increasing the batch size in a variety of models and tasks. They observed that, beyond a certain point, further increase in the batch size does not lead to reductions in training time. Researchers have proposed adapting the learning rate as the batch sizes increase [47]. Nevertheless, thesetechniques are model-specific and require significant tuning and expertise [32, 57, 95]. DeepPool [78] demonstrates convergence issues in distributed setups with large global batch sizes, when per-GPU batch size remains constant while increasing the number of GPUs. Hence, they recommend strong scaling: when scaling out training to more GPUs for large models, the optimal per-GPU batch size decreases. Similarly, Crossbow [57] exhibits the best time-to-accuracy with smaller batch sizes. These trends leave memory capacity and GPU resources underutilized. Recently, Large Language Models (LLMs) have become very prevalent, due to their high performance in a diverse spectrum of tasks. Since LLMs have exceptionally large memory capacity requirements (even with small batch sizes [60]), the opportunities to share GPUs among LLM workloads are more limited. Hence, LLMs are not our target workloads for GPU sharing. Nevertheless, in Section 7, we discuss GPU sharing opportunities for LLMs as the sequential token generation phase of LLM inference is memory-bound and underutilizes the GPU’s compute throughput and SMs [55, 60].  3.1 Profiling the GPU utilization of DNN jobs  We profile a variety of popular DNN workloads executing on an NVIDIA V100-16GB GPU without stalls. We use batch sizes for each workload based on configurations commonly used in prior work for the same or similar GPU hardware [21, 23, 51, 54, 84, 97]. We use the Nsight Compute tool to profile the compute and memory utilization of individual kernels, and get a kernel execution trace using the Nsight Systems tool. By aligning each kernel’s start and end points with resource profile information, we generate resource utilization traces for the entire workload (e.g. Figure 1). We then compute the average utilization across the whole workload. Across all workloads, we observe that GPU compute throughput and memory bandwidth utilization are bursty, as shown in the example of Figure 1, and low on average,  Figure 4. Compute vs. memory intensive kernels for model inference request (left) and model training minibatch (right).  as summarized in Table 1. Compute throughput utilization ranges from 18% to 72%, even though up to 95% of SMs can be “busy” on average (an SM is busy if it executes at least one warp). Furthermore, workloads consume only 21-49% of GPU memory bandwidth and 7-53% of GPU memory capacity. When analyzing DNN kernel execution traces, we observe that GPU compute utilization spikes often occur at different points in time than memory utilization spikes. DNN workloads consist of many kernels with different resource requirements. Figure 4 classifies each workload’s kernels as compute-intensive (performance is bounded by GPU compute throughput) or memory-intensive (performance is bounded by GPU memory bandwidth).4 Kernels typically execute for 10s to 100s of μs (for inference) or 100s to 1000s of μs (for training). As kernels from an individual DNN job execute sequentially due to data dependencies, when a kernel saturates GPU compute or memory bandwidth, it often leaves other GPU resources idle for short periods of time.  4Some kernels are labeled unknown because the NSight Compute tool does not provide a Roofline analysis for all kernels.
Kernel pairs Sequential Collocated Speedup Conv2d-Conv2d 2.59 ms 2.63 ms 0.98× BN2d-BN2d 1.78 ms 1.65 ms 1.08× Conv2d-BN2d 2.15 ms 1.52 ms 1.41× Table 2. Toy experiment collocating Conv2d (computeintensive) with BN2d (memory-intensive) kernels. Collocation leads to significant speedup over sequential execution when kernels have opposite resource intensity.  3.2 Exploring GPU kernel collocation  A promising way to improve GPU utilization is to collocate kernels with opposite resource intensity. While overlapping kernel execution within a DNN job is limited due to data dependencies, we can collocate kernels from different jobs. We conduct a toy experiment with a compute-intensive kernel Conv2d (a 2D convolution) and a memory-intensive kernel BN2d (a 2D batch normalization), commonly used in vision models. With a batch size of 32, Conv2d executes in 1.35 ms, consuming 100% of SMs on a dedicated V100 GPU. BN2d executes in 0.93 ms, consuming 40% of SMs. Table 2 shows the execution time when executing the kernels sequentially (on a single CUDA stream) and concurrently (on separate CUDA streams). Collocating two Conv2d kernels is not beneficial, as the two kernels compete for GPU SMs, ultimately running sequentially. Collocating two BN2d kernels leads to a small speedup compared to sequential execution. Despite each kernel only consuming 40% of the GPU’s SMs, their memory-intensive nature causes significant interference. In contrast, collocating a Conv2d with a BN2d kernel reduces aggregate latency by 1.41× compared to sequential execution, since they have different resource demands. Conv2d consumes 89% and 20% of GPU compute throughput and memory bandwidth, respectively, while BN2d has 14% compute throughput and 80% memory bandwidth utilization. Takeaway: Individual DNN jobs consist of kernels with various compute and memory requirements, as shown in Figure 4. Since kernels need to execute sequentially, due to data dependencies, they often underutilize GPU’s compute and memory bandwidth. Sharing GPUs between DNN jobs is necessary to maximize utilization. Our toy experiment, described in Table 2, has shown that spatial collocation is most effective for kernels with opposite compute vs. memory intensity. Since DNN jobs consist of both compute- and memory-intensive kernels (Figure 4), colocating oppositeprofile kernels from different DNN jobs would help increase utilization, while minimizing interference.  4 Related Work on GPU Sharing  We summarize current approaches for GPU sharing and discuss why they are not sufficiently fine-grained or interferenceaware to make use of GPU resources that a high-priority DNN job may underutilize for ∼100s of μs at a time.  Temporal sharing. Temporal sharing techniques timeslice the GPU by context switching between multiple jobs to improve utilization. Prior systems focus on multiplexing multiple DNN models per GPU whose collective state does not fit in GPU memory. Hence, the main challenge these systems address is efficiently swapping state as requests for particular models arrive. Gandiva [102] uses a suspend-and-restart mechanism to transfer state between host and GPU memory during context switches. Salus [104] reduces context switching by optimizing which state should remain on the GPU. Clockwork [49] serves thousands of DNNs per GPU with predictable latency by determining upfront whether the GPU can meet the request deadline based on the expected time to load/unload DNN state and run inference. Antman [103] dynamically adjusts job memory allocations to enable more efficient cluster-level job collocation per GPU for temporal sharing. Transparent GPU Sharing (TGS) [100] enables application-agnostic temporal GPU sharing for containerized workloads. However, these systems still execute one job at a time. As discussed in §3, this underutilizes GPUs as an individual DNN job’s kernels often do not consume all GPU compute units and memory. The goal of our fine-grained, interference-aware sharing is to fill spare GPU capacity for such jobs. Our work complements the above approaches, which efficiently swap state to fit more models per GPU. Spatial sharing. Spatial sharing mechanisms enable jobs to simultaneously use different regions of a GPU [106]. NVIDIA Multi-Instance GPU (MIG) [12] offers coarse-grained GPU partitioning, but lacks the agility to opportunistically harvest resources that are underutilized for short time slots. MIG partitions take 100s of ms to create and models take 10s of seconds to resume execution from checkpoints after a new partition is created [65]. NVIDIA Multi-Process Service (MPS) [25] enables multiple processes to run in parallel on a GPU, but leads to high interference, as processes freely share caches, compute, and memory resources (see Figure 2). REEF [50] schedules kernels at a fine granularity based on their size and priority, and is designed to collocate high and low priority inference jobs. Zico [67] and Tick-Tock [94] collocate training jobs on GPUs by scheduling forward and backward passes to minimize total memory consumption. However, none of these approaches co-schedule kernels based on their compute and memory profiles, which we showed in §3 is critical to minimize interference while maximizing GPU utilization.  5 Orion  We propose Orion, a fine-grained, interference-aware GPU scheduler. Orion’s goal is to maintain high performance for a high-priority job while using spare GPU resources for besteffort jobs. Orion is transparent to end users and requires no API changes. We implement Orion as a dynamically linked
library that controls GPU operations submitted by an application framework (e.g., PyTorch). As shown in Figure 5, Orion intercepts GPU operations submitted by each client and buffers the operations in per-client software queues. Operations include GPU kernels (e.g., convolution, batch normalization) and memory management operations (e.g., memory allocations, memory copies). Orion submits operations from per-client software queues to the GPU hardware using the scheduling policy described in §5.1, leveraging kernel characteristics collected during an offline workload profiling phase, described in §5.2. Orion operates on the level of a single GPU device. In distributed DNN job deployments, a separate instance of Orion runs per GPU device.  5.1 Orion Scheduler  We describe Orion’s GPU kernel scheduling policy (§5.1.1) along with the mechanisms we use to implement the scheduling policy (§5.1.2) and manage GPU memory (§5.1.3). Orion’s policy is developed and evaluated with closed-source GPUs in mind, which do not allow users to control the physical placement of the kernels in the SMs, or preempt kernels after submission.  5.1.1 GPU Kernel Scheduling Policy. The pseudo code in Listing 1 shows Orion’s scheduling policy, assuming for simplicity that two clients share the GPU: a high priority client (client_hp) and a best-effort client (client_be). Orion generalizes to an arbitrary number of best-effort clients by serving clients round-robin. Orion executes the run_scheduler method to continuously poll each client’s software queue (lines 4-6). If a kernel from the high-priority job is present (line 7), Orion submits it directly to the GPU hardware on a dedicated GPU stream (line 8). The GPU hardware executes kernels submitted to the same stream sequentially, thus respecting data dependencies. If a kernel from a best-effort job is present (line 10), Orion executes schedule_be() to decide if it is currently suitable to launch the kernel on the GPU. In order to reduce interference between the high-priority and best-effort tasks, Orion takes into account the compute and memory profiles of the high-priority and best-effort kernels, as well as the Streaming Multiprocessor demands and duration of the best-effort kernels. A best-effort kernel is suitable to schedule if there is no high-priority task ongoing. The best-effort kernel is also suitable to schedule if it is sufficiently small (in terms of the number of SMs it requires) and if the kernel has an opposite resource profile (compute vs. memory bound) compared to the high-priority kernel (lines 27-29). Orion considers the number of SMs (in addition to the compute/memory resource profiles) since large best-effort kernels might occupy all the SMs of the GPU and starve high-priority kernels. By default, we set SM_THRESHOLD to the total number of SMs on the GPU device, however, this parameter can also be tuned dynamically to increase utilization while monitoring high-priority  Model profiles Scheduler  OFFLINE ONLINE High priority  Client 1  Best effort Client 2  Best effort Client 3  Best effort  Client 4 PyTorch  Orion  (CUDA/C++)  CC  CM  M  MM  GPU  DNN workload  Profiler  Figure 5. Orion system architecture.  job performance. For example, when the high-priority job is a throughput-oriented job, such as training, we find that SM_THRESHOLD can be increased for more aggressive collocation. Tuning is done by monitoring the throughput of the high-priority job and adjusting the SM_THRESHOLD with binary search, using the maximum number of SMs needed by any kernel of the best-effort job as the max value and zero as the min value in the search. Kernels with unknown resource profiles tend to be very short-running, hence Orion optimistically allows these kernels to execute with compute or memory-bound kernels. To minimize interference despite the asynchronous nature of GPU kernel execution and the lack of kernel preemption software available to the users, Orion checks one additional condition before submitting a best-effort kernel to the GPU hardware (line 12). Orion keeps track of best-effort kernels that are still outstanding on the GPU (i.e., submitted but not yet completed) and their expected duration, obtained from an initial profiling phase (described in §5.2). Orion will only launch the best-effort kernel if the expected total duration of the outstanding best-effort kernels is not already close to the high-priority job request latency, if the high-priority job is an inference workload, or iteration duration, if the high-priority job is a training workload. Throttling longrunning sequences of best-effort kernels is necessary since as soon as they start executing on the GPU, they cannot be preempted even if high-priority kernels get submitted. DUR_THRESHOLD is a tunable percentage of the high-priority job request latency. We explore Orion’s performance sensitivity to DUR_THRESHOLD (see §6.4) and empirically set the default percentage to 2.5%. Finally, in line 19, Orion submits the best-effort kernel on a separate GPU stream and keeps track of the outstanding besteffort kernel durations. In the case of multiple best-effort clients, Orion uses a separate GPU stream for each client.  5.1.2 Scheduling Implementation Mechanisms. Orion uses two key mechanisms to implement the policy. GPU stream priorities. Closed-source GPUs, such as NVIDIA GPUs, do not expose the hardware scheduler to
1 def run_scheduler(client_q_hp , client_q_be): 2 be_duration = 0, be_submitted = Event() 3 hp_task_running = False 4 while True: 5 op_hp = client_q_hp.pop() 6 op_be = client_q_be.peek() 7 if (op_hp != None): 8 launch_kernel(op_hp , stream_hp) 9 hp_task_running = True 10 if ( op_be != None ) : 11 schedule = schedule_be ( op_hp , op_be ) 12 if ( be_duration > DUR_THRESHOLD ) : 13 if ( be_submitted . finished () ) : 14 be_duration = 0 15 else : 16 schedule = False 17 if ( schedule ) : 18 client_q_be . pop () 19 launch_kernel ( op_be , stream_be ) 20 be_duration += op_be . duration 21 be_submitted . record ( stream_be )  22  23 def schedule_be ( op_hp , op_be ) : 24 schedule = False 25 if (! hp_task_running ) : 26 schedule = True 27 else if ( op_be . sm_needed < SM_THRESHOLD 28 and have_different_profiles ( op_hp , op_lp ) ) : 29 schedule = True 30 return schedule  Listing 1. Orion scheduling algorithm assuming one high priority and one best-effort client.  software. However, we can influence the behavior of the hardware scheduler by using streams priorities [15]. The GPU hardware prioritizes scheduling an incoming kernel from a high-priority stream, even when low-priority kernels are pending (i.e. not executing yet). However, there is no guarantee that thread blocks from a high-priority stream will preempt the execution of a kernel running on a stream with a lower priority [7]. The lack of preemption is why we throttle best-effort kernels with the DUR_THRESHOLD check. CUDA events. CUDA Events [18] provide a way to monitor the progress of each stream in the GPU without expensive stream synchronization operations, which would block the CPU scheduler thread. Upon submitting a best-effort kernel, Orion records its submission in a CUDA event (line 21 in Listing 1). The GPU sets the event status as "finished" after the submitted kernel completes. Orion uses cudaEventQuery to query the status of the best-effort stream without blocking.  5.1.3 Memory Management. Orion uses the policy in §5.1.1 to schedule GPU kernels on SM units. In contrast, memory allocation and memory copy operations consume only  CPU-GPU PCIe bandwidth. In our current design, Orion directly submits memory operations to the GPU. We plan to extend our current implementation with techniques that manage PCIe bandwidth interference [39]. Orion could schedule each cudaMemcpy operation by considering its PCIe bandwidth requirements and current bus bandwidth utilization. Orion maintains application semantics for all memory operations. For blocking operations, like cudaMemcpy and cudaMemset, Orion blocks until the operation completes on the GPU. For asynchronous memory operations, such as cudaMemcpyAsync, clients continue executing after Orion intercepts the operation. For memory operations that cause device synchronization (e.g., cudaMalloc, cudaFree), Orion synchronizes all clients to avoid invalid memory accesses. The current implementation of Orion assumes that the cluster manager chooses to collocate jobs that fit in GPU memory, as assumed in other works like REEF [50]. Orion is orthogonal to and hence can be combined out-of-thebox with existing mechanisms for GPU memory swapping, such as making use of the NVIDIA Unified Memory mechanism [5], or more sophisticated swapping mechanisms as the ones proposed in Salus [104], PipeSwitch [35], ClockWork [49], and vLLM [60]. We intend to integrate layer-bylayer offloading [83] to Orion. This involves maintaining the high-priority task on the GPU while gradually swapping layers of best-effort job(s) in and out of the GPU if the whole model(s) do not fit in the remaining GPU memory. As GPUCPU interconnects are becoming faster and faster [24], we expect that swapping will have lower overhead.  5.2 Workload Profiling  Orion’s scheduling policy requires information about the compute vs. memory intensity of each kernel, the expected execution time and SM requirements of each best-effort job kernel, and the request latency of high-priority jobs. Before execution, Orion profiles each DNN workload offline and generates a file containing profile information for each kernel in the model. The Orion scheduler loads the profiling information in an in-memory lookup table, indexed by unique kernel ID.  Kernel latency and resource profiles. Orion uses the Night Compute [26] and Nsight Systems [27] tools from NVIDIA to collect the compute throughput, memory throughput, and execution time of each kernel. We use the roofline analysis in Nsight Compute, which classifies a kernel as compute-bound or memory-bound. Since the tool does not include roofline analysis for all kernels, we further classify a kernel as compute or memory bound if its compute throughput or memory bandwidth utilization is over 60%, respectively, as recommended by the Nsight Compute tool. If both compute throughput and memory bandwidth utilization are below 60% and roofline analysis is not available for the kernel, we classify its resource profile as unknown. In practice,we find that the unknown kernels mostly occur in the update phase of a training iteration, are very small, and introduce negligible interference when collocated with other jobs. Thus, Orion allows best-effort kernels of unknown resource profiles to be collocated with any high-priority kernel. Kernel SM requirements. For each kernel in the besteffort jobs, Orion uses the Nsight Compute tool to get the number of blocks, the number of threads per block, the number of registers per thread, and the amount of shared memory that the kernel requires. For each kernel k, we first determine blocks_per_smk , which is the number of blocks that can be supported per SM on the target GPU architecture for that kernel. blocks_per_smk can be limited by the number of threads, the number of registers, or the amount of shared memory available per SM that the kernel k requires. We then calculate the number of SMs required per kernel as: sm_neededk = ceil(num_blocksk /blocks_per_smk ).  Request latency. To determine the DUR_THRESHOLD parameter, which Orion uses to throttle best-effort kernel launches based on their duration relative to high-priority request execution, Orion must also profile high-priority request latency, when the job is running alone in a dedicated GPU. For inference jobs, a request refers to a single batch of inference requests. For training jobs, a request refers to a single training iteration.  5.3 Integration in DNN framework  Orion’s scheduling policy is agnostic to the application framework. Orion is dynamically linked to the DNN framework and is transparent to end users. PyTorch Prototype. For our prototype, we implement Orion in PyTorch [79] in 3000 lines of C++/CUDA code. In native PyTorch, client applications launch kernels using the CUDA runtime API [14] and libraries like CUBLAS [19] and CUDNN [20], which provide high-performance implementations for common DNN operations. Orion intercepts CUDA kernel launches by overriding them with wrapper functions, which submit the necessary information (kernel identifier and arguments) to the per-client software queues. Orion currently implements wrappers for memory management operations (cudaMalloc, cudaMemcpy, cudaMemset, cudaFree, etc) and kernel launch operations (cudaLaunchKernel) from the CUDA runtime API, as well as CUDNN and CUBLAS functions for convolution, batch normalization, and matrixmatrix multiplication. These wrappers are sufficient to support all the DNN workloads in our evaluation, though more can be added. Intercepting operations and managing the perclient software queues is lightweight. The overhead of using Orion’s wrappers is less than 1%, as we show in §6.5. In our current prototype and evaluation, client applications and the Orion scheduler run as different threads of the same process, enabling in-process memory sharing and fast communication. Orion can also be used for applications executing as different processes. In this case, Orion executes  as a separate process and clients submit kernels to queues in shared memory regions. This requires the GPU to support concurrent access from multiple processes, such as NVIDIA’s MPS feature [25].  6 Evaluation  We evaluate Orion to answer the following key questions:  • How does Orion’s performance compare to other GPU sharing approaches? • What are the cost and GPU utilization benefits of using Orion compared to dedicating GPUs for each job? • Which aspects of Orion’s scheduling policy contribute most to performance benefits? • How does Orion generalize to a new GPU architecture? • How does Orion scale to multiple best-effort clients? • What are the overheads of Orion’s kernel profiling and kernel launch interception mechanisms?  6.1 Methodology  Experiment testbed. We evaluate Orion on an NVIDIA V100-16GB GPU using a Google Cloud n1-standard-8 VM, which has 8 vCPU cores and 30 GB of DRAM. We use PyTorch 1.12 with Python 3.9 and CUDA 10.2. We also show that Orion generalizes to other GPU architectures by evaluating Orion on an A100-40GB GPU using an a2-highgpu-1g VM, with CUDA 11.3. For all experiments, we ensure jobs execute with no data preprocessing or communication bottlenecks. Hence, we evaluate Orion’s ability to improve GPU utilization while minimizing interference in the most challenging setting, where each individual job maximizes its own GPU utilization. We repeat each experiment three times. Workloads. We consider three common GPU sharing use cases. First, we collocate a high-priority, latency-sensitive inference job with a best-effort training job (inf-train). Next, we collocate high-priority and best-effort training (traintrain). Finally, we collocate a high-priority, latency-sensitive inference job with best-effort offline inference jobs (inf-inf ). For each use case, we consider popular DNN models from computer vision and natural language processing (NLP) domains. ResNet50, ResNet101 [51] and MobileNet-v2 [84] are representative vision models. We use their TorchVision implementations [28]. BERT [43] and Transformer [92] are representative NLP models. We use the implementations from NVIDIA [22]. We use full-precision for both training and inference. Table 1 summarizes the batch size for each workload. We match batch sizes to those used in prior works on the same or comparable GPU platforms [21, 23, 51, 54, 84, 97]. We consider uniform and Poisson request arrival distributions for inference jobs. A uniform distribution is representative of application domains such as autonomous driving (e.g., where cameras detect obstacles [36, 37]), whereas Poisson arrivals are representative of event-driven, real-time DNN applications (e.g., speech recognition [50, 52]). We selec
the mean request arrival rates (shown in Table 3) to match the mean invocation request rates of the top 20 most frequently executed functions in the Microsoft Azure Functions trace [87], as used in other works [49, 105] to stress-test GPU collocation scenarios. For the vision models, we also use an inference trace collected from a real object detection model deployment in the Apollo autonomous driving system [36]. This inference trace is from the DISB inference serving benchmark [17], first used to evaluate REEF [50]. For experiments with the Apollo trace, we use the trace’s invocation timestamps for the high-priority inference job and assume uniform request inter-arrival for collocated best-effort inference jobs. Meanwhile, training jobs submit requests in a closed loop. Baselines. We compare Orion to temporal sharing, which time-slices the GPU by executing one job’s request at a time, while prioritizing the high-priority job. We also compare Orion to NVIDIA MPS [25] and GPU Streams [3] spatial sharing mechanisms. GPU Streams allow multiple client applications to share a GPU, as long as they are part of the same process. Hence, for this baseline, we run each DNN application client as a separate thread, which submits requests to a separate CUDA stream. We assign a high-priority stream to the high-priority job, and a default-priority stream to each of the best-effort jobs. MPS is a feature in NVIDIA GPUs with compute capability 3.5 or higher, which enables multiple processes to spatially share the GPU. We also compare to the state-of-the-art REEF [50] GPU sharing policy. REEF is originally developed for AMD GPUs, which allows users to preempt kernels during execution. For NVIDIA GPUs, the authors proposed a restricted version of kernel preemption, REEF-N, which allows high-priority kernels to bypass best-effort kernels in software queues before submission to GPU. Since only the AMD GPU version of REEF is currently open source, we implement REEF-N and the kernel selection rules from the original paper, which schedule kernels based on their size (number of SMs) and expected latency. We use a software queue size of 12 kernels, based on discussions with the REEF authors. While REEF was primarily designed to collocate inference jobs (since training jobs include non-idempotent kernels that update model state), REEF-N is safe to use for training jobs as kernel  execution is never preempted. Hence, we compare Orion to the REEF-N policy for all collocation use cases. For training job collocation (train-train), we compare Orion to Tick-Tock [94], which offsets the forward and backward passes of training minibatch iterations to minimize aggregate memory usage and reduce interference. Zico [67] also implements this approach. Since neither system has an available open-source implementation for PyTorch, we implement the approach based on the papers. As a performance upper bound, we measure the latency and throughput of each workload on a dedicated GPU. The Ideal baseline has latency equal to the high-priority job’s latency with no collocation and throughput equal to the sum of high-priority’s and best-effort jobs’ dedicated GPU throughput. This baseline is a lower bound for latency and an upper bound for throughput.  6.2 Performance and cost benefits  We evaluate inf-train, train-train, and inf-inf collocation use cases with two DNN clients at a time: one high-priority and one best-effort. For the high-priority inference job, we report the p99 latency. We observe similar trends for all baselines for the p50 and p95 latency, so we omit the respective plots for brevity.  6.2.1 Inference-Training. Figure 6 shows the p99 latency and aggregate throughput of a high-priority inference job with Apollo trace request arrivals, collocated with a besteffort training job. Each bar represents the performance of a high-priority inference workload (labeled on the x-axis), averaged across experiments that collocate each one of the DNN training jobs from Table 3. The standard deviation bars show how inference latency varies across the five different collocated training jobs. Figure 7 shows results for a similar inf-train setup, but assuming Poisson arrivals for the high-priority inference job. Temporal sharing leads to high tail inference latency, despite prioritizing inference requests. An incoming inference request must wait for any ongoing training iteration to complete before it starts executing, resulting in high queuing delays. Spatial sharing mechanisms improve performance by parallelizing request execution across multiple streams. However, GPU Streams and MPS mechanisms only maximize aggregate throughput and do not prioritize the latencysensitive inference job, leading to high tail latency. MPS generally achieves lower latency than Streams, since multiprocessing is more efficient than multi-threading in Python applications. Clients in the Streams baseline run as different threads, hence they contend for the Python global interpreter lock [10]. The REEF policy is also unable to maintain low p99 inference latency, as it lacks interference-aware scheduling and does not sufficiently throttle the best-effort training job. On average, REEF’s inference latency is 3.44× higher than ideal for the Apollo trace experiments and 2.5× higher than ideal for the Poisson arrival experiments. In contrast, Orion keeps p99 latency of the high-priority inference job within 14% of the ideal latency, which is 2.33× lower than REEF, on average. Orion also achieves low variance in inference latency across collocations with different models. At the same time, Orion increases aggregate throughput by up to 1.3× and 2.3× compared to inference throughput on a dedicated GPU, for the Apollo and Poisson experiments, respectively. Table 4 shows the cost benefits of using Orion to collocate a Poisson arrival inference job with different training jobs on a single GPU, compared to dedicating separate GPUs for each job. We calculate the cost savings as:  cost savings = 2 GPU · JCTdedicated  1 GPU · JCTcollocated  = 2 · T hroughputcollocated  T hroughputdedicated  where JCT is the job completion time. Dedicated and collocated scenarios use 2 GPUs and 1 GPU, respectively. Throughput is the inverse of job completion time. Overall, Orion achieves 1.26× to 1.49× cost savings. We also measure Orion’s impact on GPU utilization. Figure 8a plots V100 GPU Compute Throughput utilization over time for a ResNet50 inference job running alone, while Figure 8b shows utilization when using Orion to collocate the inference job with a best-effort ResNet50 training job. The inference job submits requests with a uniform distribution at 100 requests per second. Orion fills in the fine-grained low-utilization periods of the inference job, increasing the average Compute Throughput utilization from 7% to 36%. The remaining periods of low utilization in Figure 8b correspond to times when memory copy operations execute from the host CPU to GPU. The GPU hardware cannot schedule kernels during memory copies [1]. Similarily, Figure 9a and 9 plot the GPU memory bandwidth utilization of the inference job running alone and colocated with training, respectively. In that case, Orion improves Memory Bandwidth Utilization from 10% to 47%. Finally, Orion improves SM utilization from 11% to 49%.  6.2.2 Training-Training. Figure 10 shows average aggregate throughput when collocating high-priority and besteffort training jobs. With MPS and Streams, high-priority job throughput is, on average, 1.7× less than the job’s throughput on a dedicated GPU, due to interference. MPS achieves up to 6% higher throughput than Streams due to processbased parallelism. Tick-Tock exhibits the lowest throughput among all baselines, mainly due to synchronization at the beginning and end of the forward and backward passes. This causes the fastest job to wait for the slowest one, reducing the throughput of the high-priority training job by 1.93×.  (a) ResNet50 Inference alone  (b) ResNet50 Inference + ResNet50 Training  Figure 9. Inference Memory bandwidth utilization on a dedicated GPU vs. collocated with training.  Figure 10. Average throughput of high priority (bold bar) and best-effort training jobs (faint bar).  In contrast, REEF maintains high-priority job throughput within 8% of ideal. However, Figure 10 shows that REEF heavily throttles best-effort kernels, as few best-effort training iterations complete. Orion achieves the best of both worlds. It maintains high-priority job throughput within 16% of ideal, while achieving up to 1.6× higher throughput compared to dedicating the GPU to the high-priority job. By collocating the best-effort job during low-utilization periods of the high-priority job, Orion effectively increases overall GPU utilization. For example, when collocating a high-priority job training BERT with a best-effort job training MobileNet, Orion increases average SM utilization, compute utilization, and memory bandwidth utilization by 1.4×, 1.34×, and 1.7×, respectively. By making progress on best-effort training jobs while serving a high-priority training job, Orion reduces the total time to complete a set of training jobs (i.e., job makespan). We compare the cost of training all examined models on a single GPU with Orion versus executing training jobs sequentially on a dedicated GPU. We run ResNet50, ResNet101, and BERT as high-priority training jobs and MobileNet-V2 and Transformer as best-effort jobs. Orion reduces the job makespan by 1.29×, leading to 1.29× cost savings as the GPU is required for less time to complete the jobs. In comparison, the MPS baseline achieves only 1.14× cost savings compared to sequential execution, and at the expense of 1.25× higher job completion time for high-priority jobs compared to Orion. Compared to REEF, Orion reduces JCT and cost by 1.29×.  6.2.3 Inference-Inference. Figure 11 shows the p99 inference latency when collocating a high-priority vision inference job with a best-effort inference job. The model on the x-axis is the high-priority model, which receives inference requests based on the Apollo trace. The best-effort inference job receives requests with uniform inter-arrival distribution. Figure 12 shows a similar experiment, but assuming Poisson arrivals for both jobs. In our inf-inf experiments, all baselines achieve similar aggregate throughput (not shown in the plots), but the tail latency of the high-priority inference job differs greatly across baselines. GPU Streams and MPS incur high p99 latency overhead: on average 1.89× higher than the ideal p99 latency and with high variance across model collocations. Since REEF does not take into account the compute versus memory intensity of the kernels it schedules, its p99 latency for the high-priority job is 1.86× and 1.25× higher than the ideal case, for the Apollo and Poisson experiments, respectively. In contrast, Orion keeps the high-priority inference p99 latency within 22% and 15% of the ideal latency for the Apollo and Poisson experiments, respectively, while increasing aggregate inference throughput by up to 2× (for Apollo) and 7.3× (for Poisson) compared to dedicating the GPU to the high-priority job only. Overall, Orion provides cost savings of 2× for 2-client inf-inf use cases compared to the dedicated GPU case, since Orion serves the models on a single GPU instead of one for each job.  Figure 12. Inference-Inference (Poisson): p99 latency of high-priority model (on x-axis) averaged across collocation experiments with all other models.  6.3 Generalizing to other GPUs and more clients  As GPU hardware resource capacity continues to scale with each new generation [11], more tenants can be collocated. To show that Orion can scale to multiple clients and generalize to other GPU architectures, we evaluate Orion with 5 inference clients sharing a A100-40GB GPU. Figure 13 plots p99 latency of the high-priority inference job (labeled on x-axis) collocated with 4 best-effort inference jobs serving the other models in Table 3, all with Poisson request arrivals. We show standard deviation across three runs of the same experiment. We compare MPS and REEF with Orion. We omit temporal sharing and Streams baselines due to their poor performance (tail latency is 3 orders of magnitude higher than ideal). MPS leads to 2.2× higher p99 latency than ideal. Although employing REEF’s fine-grained scheduling policy helps reduce tail latency, it is still 21% higher than ideal. In contrast, Orion’s interference-aware policy and control mechanisms keep the p99 latency of all workloads within 9% of ideal, showcasing Orion’s ability to generalize across GPU generations and scale to multiple clients.  6.4 Performance analysis breakdown  In Figure 14, we analyze which aspects of Orion’s policy contribute most to performance benefits. We show results for the inf-train use case with Poisson inference arrivals. Our conclusions apply to the other use cases as well. We start by simply assigning each client to a different CUDA stream, each with the default priority. The GPU Streams bar in Figure 14 shows this approach has high latency. The Stream Priorities baseline shows that using the highest CUDA priority for the stream of the high-priority inference job helps reduce the p95 latency by up to 25%. Adding the first component of Orion’s policy, which schedules besteffort job kernels based on their compute-memory resource profiles, reduces the p95 latency by an additional 48%. Finally, taking into account the sizes (number of SMs) of the kernels reduces the latency by up to 54% on top of the Compute/Mem
profiles baseline. Hence, compute/memory-aware and sizeaware scheduling are roughly equally important. We then check whether stream priorities are essential to the Orion system after applying compute/memory profiles and kernel size-based scheduling. The stream priority mechanism has only marginal improvements at this point, hence Orion can also be used in settings where the GPU hardware does not support stream priorities (e.g., in MPS mode [46]). We also tune DUR_THRESHOLD. We find that Orion has stable performance for DUR_THRESHOLD values below 3%. Linear increases in DUR_THRESHOLD beyond 3% lead to approximately linear decrease in high-priority job performance, due to less throttling of best-effort kernels. For example, when collocating ResNet101 inference with best-effort training, inference latency is 23ms, 26ms, and 30ms for DUR_THRESHOLD values of 10%, 15%, and 20%, respectively, while best-effort training throughput is 8.7, 9.26, and 9.75 iterations/sec. Users can tune DUR_THRESHOLD based on high-priority job service level objectives. We use 2.5% in our experiments.  6.5 Overheads  Kernel launch interception. We measure the execution time of each inference and training job on a dedicated GPU using Orion’s kernel interception mechanism to directly schedule kernels. Compared to native PyTorch, Orion’s overhead remains less than 1% across all jobs.  Kernel resource profiling. We use the Nsight Systems (NSYS) and Nsight Compute (NCU) tools from NVIDIA to profile the first 10 mini-batches of a training job or 10 requests of an inference job. The Nsys tool adds up to 5% overhead in the iteration time. The NCU tool performs a much more detailed resource analysis for each kernel (e.g. cache misses, warp scheduler statistics), and the profiling time is proportional to the number of kernels. In our experiments, it takes ∼2-5 seconds per kernel. Since profiling is offline, the tools do not affect the actual job execution.  Figure 14. Orion performance analysis breakdown for inftrain, showing which aspects benefit tail latency the most.  7 Discussion  Cluster manager co-design. Orion is currently implemented as a per-GPU scheduler. In the future, we plan to explore co-design with cluster management. By using each job’s compute and memory intensity kernel profiles, the cluster manager can place jobs with complementary resource profiles on the same GPU(s) to maximize resource utilization and mitigate interference.  Software/hardware co-design. Optimizing GPU utilization and performance is challenging with the limited interface that GPU hardware currently exposes to host software. We draw inspiration from the OpenSSD [59] platform, which enables research in Flash storage device controller hardware and host software co-design by exposing a lower-level interface to software. Similar to how software/hardware co-design can minimize interference on shared SSDs [31, 63, 64, 69], enabling software/hardware co-design for GPU scheduling (e.g., controlling the placement of GPU kernels across SMs) can allow applications to tune end-to-end performance on shared GPUs. This is particularly important as recent trends in GPU programming increasingly offload more and more scheduling to GPU hardware. For instance, CUDA graphs [6] schedule entire graphs of kernels in the GPU with a single CUDA API call to reduce CPU launching overhead. In this context, Orion’s scheduling policy could be implemented either at the GPU driver or GPU scheduler level, to interleave kernels from multiple graphs while minimizing interference. GPU cache interference. We currently do not consider GPU cache interference. NVIDIA tools provide cache miss statistics [26], which can be used to infer more specific profiles of kernels and model interference more accurately. Security. We assume the clients sharing a GPU are in the same trust domain, which is a reasonable assumption in DNN clusters operated by the same organization [98, 99]. Hence, Orion minimizes performance interference, but does not guarantee secure isolation between untrusted clients sharing a GPU. Trusted execution environments for heterogeneous hardware are an active area of research [93].
Applicability to Large Language Models (LLMs). We plan to further investigate Orion’s applicability to Large Language Models [38, 90]. Previous works [55, 60] have shown that the token generation phase of LLM inference, which happens sequentially, token after token, is memory-bound, while underutilizing GPU’s compute throughput and SMs. Thus, we can employ Orion’s resource-aware scheduling policy to colocate LLM inference with computationally intensive workloads. However, the large size of LLMs [38], as well as the Key-Value cache, used to speedup token generation [81], significantly inflate the memory requirements of LLM inference. Therefore, when colocating with other workloads, additional memory swapping mechanisms must be employed. As outlined in section 5.1.3, we plan to enhance Orion with existing DNN swapping mechanisms. One such mechanism is PagedAttention [60] which offers dynamic allocation and swapping for LLM inference and can be seamlessly integrated with Orion.  8 Conclusion  Orion is a GPU scheduler that transparently schedules tasks from multiple clients sharing a GPU at the granularity of individual operators. By considering the size, compute, and memory profiles of each operator, Orion reduces interference to maintain high performance for a high-priority workload while saving up to 1.49× in cost by making progress on collocated best-effort jobs (compared to dedicating GPUs to individual jobs). Unlike other GPU sharing techniques, Orion schedules at the granularity of individual GPU kernels, enabling it to leverage spare GPU resources available for short time periods (e.g., 10s to 1000s of μs) during DNN job execution. This approach significantly reduces tail latency for high-priority jobs compared to prior GPU sharing 
PPoPP ’24, March 2–6, 2024, Edinburgh, United Kingdom Ismet Dagli and Mehmet E. Belviranli  We investigate concurrent execution on shared memory SoCs through a case study. In a typical loop running on autonomous systems, VGG-19 [58] and ResNet101 [18] can be used in tandem for vision (i.e., perception) tasks. Since the remaining tasks in the autonomous loop depend on the completion of these two DNNs, utilizing all computational resources in the SoC for these DNNs is expected to reduce the total latency of the system. Fig. 1 illustrates three different ways of executing two DNNs on NVIDIA Xavier AGX SoC. In Case 1, DNNs are serially executed on the fastest DSA, which is the GPU, resulting in 11.3ms of cumulative latency. However, this method leaves DLA idle, thereby under-utilizing the system resources. The first approach can be improved by a naïve concurrent execution, as shown in Case 2. In this scheme, VGG-19 is run on GPU, and ResNet101 is mapped to DLA, resulting in 10.6ms cumulative latency with a slight improvement. However, the speed-up obtained remains limited due to two reasons: (1) DLA takes longer to execute, leaving GPU idle towards the end, and (2) when GPU and DLA operate together, they contend for shared memory and slow down. For more efficient execution of concurrent DNNs, we need a finer-grained, i.e., layer-level, mapping of the DNNs to DSAs. Case 3 depicts an ideal case where layers in both DNNs are divided into two groups after layers #28 and #95, respectively. For each DNN, the execution is switched between two DSAs at the boundary of corresponding layer groups (i.e., transition point). While seemingly non-intuitive, this approach considerably improves the cumulative latency and increases the overall system utilization. This is due to a careful partitioning and mapping of layers to GPU and DLA in a way that: (1) the shared memory contention across concurrently running layers is minimized, (2) neither of the DSAs is left idle, and (3) the overhead of switching between accelerators between two layers is minimized. However, finding such partitioning is not trivial, and the state-of-the-art approaches (detailed in Section 2) fail to provide a holistic approach to perform this partitioning optimally.  In this study, we propose HaX-CoNN, a multi-accelerator and contention-aware execution scheme for collaboratively and concurrently running DNNs on shared memory SoCs. HaX-CoNN is centered around characterizing common layers in DNNs according to their DSA-specific performance and identifying how they are affected by shared memory contention. Leveraging decoupled performance and contention characterization at a layer-level, HaX-CoNN exploits distinct capabilities of each DSA in the system by deciding whether the execution of the next layer in the DNN should transition to another DSA or not. HaX-CoNN uniquely finds an optimal mapping between the layers and DSAs in the system by formulating the problem as a set of constraint-based linear equations and utilizing SAT solvers to find a solution. Our work makes the following contributions: • We present HaX-CoNN, a contention-aware, multi-accelerator execution scheme that maximizes compute utilization and  minimizes the overall latency of concurrently running DNNs on shared memory SoCs. • We propose a generalized and formal layer-to-accelerator mapping approach for concurrently running DNNs. We demonstrate that SAT solvers can be utilized to produce optimal schedules for multi-accelerator execution.  • We build a new contention modeling approach which significantly reduces profiling search space by decoupling performance measurement and the slowdown. • We present D-HaX-CoNN, a dynamic runtime adaptation of SAT solver-based optimal schedule generation for dynamically changing workloads. • We evaluate HaX-CoNN and D-HaX-CoNN on NVIDIA AGX Orin, Xavier AGX, and Qualcomm Snapdragon 865 SoCs. Our results show that HaX-CoNN can provide latency and throughput improvements up to 32% and 29%, respectively, over greedy-scheduling based approaches.  2 Related Work  Concurrent DNN execution: Several studies [10, 32, 33, 37, 43, 69] propose scheduling techniques for the concurrent execution of multiple DNNs. Two of them focus on multiDNN inference on SoCs: Herald [37] introduces a mapper to optimize hardware resource utilization across accelerators such as NVDLA [1] and Shi-diannao [14] whereas H2H [69] improves Herald by considering inter-accelerator transition costs.  Multi-accelerator scheduling: Scheduling for systems with more than one type of accelerator has recently been targeted by many studies [4, 6, 24, 28, 29, 63, 66]. Among the most relevant, Gamma [29] and Kang et al. [28] build genetic algorithms to utilize multiple accelerators for a single DNN execution while Wu et al. [66] and Mensa [6] target unique hardware for edge devices. None of these studies address contention and balancing issues with multi-DNN execution. DNN training for large-scale systems [27, 40, 42, 50], on the other hand, is outside the scope of this work.  Optimal schedule generation: Only a couple of studies create optimal schedules for multi-DSA execution. AxoNN [11] maps layers of a single DNN onto heterogeneous accelerators under an energy budget, resulting in a serial  Table 1. Feature comparison between the most related work and HaX-CoNN.  Related Work  Mensa [6]  AxoNN [11]  Pipeline [24]  OmniBoost [32]  MoCA [33]  Herald [37]  H2H [69]  HaX-CoNN  Concurrent DNNs ✗ ✗ ✗ ✓ ✓ ✓ ✓ ✓ Multi-accelerator ✓ ✓ ✓ ✗ ✗ ✓ ✓ ✓ Transition cost ✓ ✓ ✓ ✓ ✓ ✗ ✓ ✓ Memory contention ✗ ✗ ✗ ✗ ✓ ✗ ✗ ✓ Dynamic scheduling ✓ ✗ ✓ ✗ ✓ ✗ ✗ ✓ Optimal schedules ✗ ✓ ✗ ✓ ✗ ✗ ✗ ✓  244
Contention-aware Concurrent DNN Execution on Shared Memory SoCs PPoPP ’24, March 2–6, 2024, Edinburgh, United Kingdom  Memory Contention Slowdown  Overview  Concurrent DNNs  Sec 3.2  Sec 3.3  Sec 3.4 Sec 3.5  Sec 3.1  Target DSAs  Layer Characterization  Schedule Generation  Problem Formulation  Layer Grouping  Figure 2. Overview of HaX-CoNN.  execution. OmniBoost [32] uses Monte Carlo tree search by exhaustively profiling layers on CPU and GPU. Both approaches only create static schedules.  Shared Memory Contention: None of the studies mentioned so far addresses shared memory contention. MoCA [33] designs a multitenant DSA architecture with dynamic memory resource management. FAST [68] uses an integer linear programming based operator fusion technique to remedy the memory bottlenecks whereas ParDNN [51] partitions DNNs under a memory limit. However, these approaches are not adaptable to off-the-shelf multi-DSA shared memory SoCs. Table 1 provides a snapshot of what the most relevant works offer and how they compare against HaX-CoNN. Achieving the ideal execution scenario depicted in Case 3 of Fig. 1 requires holistic consideration of several factors given in the table: (i, ii) interaction and mapping opportunities created by running concurrent DNNs on different types of DSAs, (iii) the transition overhead when the execution within a DNN switches across accelerators, (iv) the slowdown caused by the shared memory contention as layers run concurrently our analysis shows that shared memory contention-unaware decisions can reduce system performance by up to 70%, as detailed in Section 5.2–, (v) support for dynamic schedules, and (vi) optimal schedule creation. The efficient and safe operation of performance critical mobile and autonomous workloads on shared memory SoCs depends on the holistic consideration of all these factors. Our experiments demonstrate that the lack of such consideration results in mispredicted performance, which in turn results in inefficient execution.  3 HaX-CoNN: Heterogeneity-aware Execution of Concurrent Deep Neural Networks  An overview of our proposed methodology is given in Fig. 2. HaX-CoNN takes the DNNs to be scheduled and the target DSAs as input and produces the optimal schedule as output.  3.1 Layer grouping  The first step involves identifying minimal layer groups to serve as atomic assignment units for DSAs. This grouping considers several factors:  1) Preserving layer optimizations: Layer/operator fusion [2, 7, 44] merges multiple layers into a single layer. Transition points during DNN execution, where we switch execution from one DSA to another, should not impede operator fusion.  Therefore, we ensure that fusible layers are grouped together and mapped to the same accelerator.  2) Input/output reformatting: DSAs typically operate in an internal hardware (HW) pipeline. If a transition from a layer mapped to such a DSA disrupts its pipeline, then an additional output reformatting operation is inserted by the execution framework. Similarly, input reformatting might be required after transitioning to that DSA. Layer groupings can be structured to avoid such formatting overheads.  3) Accelerator and software limitations: DSAs are often limited by the layer types, parameters, and batch sizes they support. DNN execution frameworks, such as NVIDIA TensorRT [48] and Qualcomm SNPE [52], ensure such constraints are followed. We identify such limitations via vendor specific API calls and our framework considers these limitations when locating valid transitions between accelerators. In this step, we group layers as follows: If transitioning to another DSA after a layer is prohibited or leads to increased overhead, the layer is grouped with subsequent layers. Otherwise, the layer is marked as a potential transition point.  3.2 Per-layer performance and transition characterization  After we identify all feasible layer groupings, hence the transition points, the next step is to characterize each layer’s (or layer group’s) performance and the overhead if an inter-DSA transition occurs after that particular layer (or layer group).  Layer characterization: Strategically assigning layers to the DSAs where they will run most efficiently has the potential to increase performance. Previous studies [6, 19, 30, 31, 37] have detailed various parameters that affect the efficiency of deep learning accelerators, such as layer type, input size, kernel size, etc. Different layers within a DNN yield varying performance speed-ups when run on a specific DSA. To illustrate and analyze this further, we conduct an experiment where we profile layer groups in GoogleNet on GPU and DLA. The results given in Table 2 show that while the DLA performs slower than GPU for all layers, the speed reduction is less severe for some layers. The fourth column lists the ratio of execution time on DLA over GPU, which varies from 1.40x to 2.02x among different layer groups. Larger performance discrepancies primarily arise because compared to DLAs, GPUs are heavily optimized for large-size matrix operations and they are capable of more effectively exploiting performance on convolution operations with larger inputs. Conversely, smaller kernels, such as those in groups 95-109 and 124-140, are better fits for the DLA’s internal on-chip buffer. Prior studies [3, 6, 11, 25, 32] show that it is feasible to characterize DNNs via a layer-centric profiling approach where commonly used layer types are profiled beforehand for different input and filter sizes. Following a similar methodology, we profile each layer or layer group on the DSAs in the system. We utilize IProfiler interface of TensorRT on NVIDIA devices [49], which reports per layer time. Profiled execution  245
PPoPP ’24, March 2–6, 2024, Edinburgh, United Kingdom Ismet Dagli and Mehmet E. Belviranli  Table 2. Execution (E) and transition (T) time of layer groups in GoogleNet  Layer Group  GPU (ms)  DLA (ms)  D/G E. Time Ratio  T. Time G to D (ms)  T. Time D to G (ms)  Memory Thr. (%)  0-9 0.45 0.75 1.65 0.056 0.15 41.97 10-24 0.19 0.34 1.80 0.075 0.13 62.21 25-38 0.31 0.45 1.44 0.062 0.08 78.49 39-52 0.18 0.37 2.02 0.011 0.03 53.41 53-66 0.16 0.31 1.98 0.055 0.03 55.70 67-80 0.17 0.33 1.96 0.024 0.04 59.24 81-94 0.21 0.31 1.50 0.058 0.05 62.60 95-109 0.25 0.35 1.40 0.030 0.06 76.12 110-123 0.16 0.27 1.66 0.024 0.07 66.95 124-140 0.24 0.36 1.49 0.007 0.05 47.96  times are then embedded into variable t, which is used in equations 2, 4, 5, and 7 in Section 3.4.  Inter-DSA layer transitions: Despite the potential performance boost offered by multi-DSA execution, transitioning between DSAs comes with a cost. This cost, crucial for accurate performance predictions and optimal scheduling, is contingent on the size of the transient data in private caches of DSAs. The output of the layer preceding the transition is flushed back to the shared memory so that the DSA where the next layer will execute on can access it. The fifth and sixth columns in Table 2 represent the time spent when the execution, after each layer group, switches from GPU to DLA and vice versa. As output data sizes decrease toward the end of layer groups, so does transition time. Notably, our experiments also reveal that some layer groups, such as 39-53 and 95-109, ending with pooling layers result in significantly less transition overhead when switching from GPU to DLA. We empirically derive the transition costs of the layers on our target set of accelerators, following the methodology outlined in [11]. To implement them, we insert MarkOutput and addInput API calls in TensorRT [49]. We then incorporate them into equations 2 and 3 in Section 3.4.  3.3 Characterizing shared memory contention  One of the core novelties of our work is its ability to account for the slowdown caused by shared memory contention. Since existing multi-DSA schedulers do not consider this when making scheduling decisions, the resulting mappings often leave the system under-utilized. However, estimating this slowdown, especially for multi-DSA systems, is not trivial. Exhaustive and peer-wise runs of all combinations of layers by colocating them are required. This will result in a factorial explosion of profiling search space and require significant profiling time [71]. To prevent this, we follow a decoupled two-step approach: we first characterize each layer’s requested memory throughput when they are run standalone. Using these throughput values, we then utilize a processor-centric slowdown model, PCCS [67], to estimate the slowdown without relying on layer-specific information. PCCS represents the slowdown  Input and Filter Size  EMC Utilization (%)  0  20  40  60  80  i1_f1  i1_f2  i1_f3  i1_f4  i1_f5  i2_f1  i2_f2  i2_f3  i2_f4  i2_f5  i3_f1  i3_f2  i3_f3  i3_f4  i3_f5  i4_f1  i4_f2  i4_f3  i4_f4  i4_f5  i5_f1  i5_f2  i5_f3  i5_f4  i5_f5  GPU DLA  Figure 3. EMC utilization by conv layers on GPU and DLA with varying input (i) and filter (f) sizes  between multiple concurrent workloads as a function of requested memory throughput and external memory traffic, and builds a piece-wise model to predict the slowdown experienced by the accelerator requesting the throughput. Built upon PCCS, our decoupled approach is performed at layer-level and separates the collection of layer-specific standalone performance profiles, as collected in Section 3.2, and the slowdown caused by concurrent execution. The last column in Table 2 lists memory throughput measurements per layer group in GoogleNet. The general pattern we observe in many DNNs is that higher input size results in higher memory throughput. We also observe that, as the filter size in convolution and pooling layers gets larger, there is a decrease in throughput due to the increasing arithmetic intensity of the underlying operation. Conventional hardware counters to monitor requested memory throughput may not be applicable for some blackbox DSAs which cannot be profiled with conventional tools. For example, NVIDIA Nsight Compute tool [8] can profile requested memory throughput on GPUs but not on DLAs. As an alternative way to methodologically solve this issue, we develop a four-step approach: 1) We first profile target layers on GPU and analyze the memory throughput for several layer types (i.e., convolution, pooling, and fully connected) and their parameters (i.e., input size filter size). Throughout their execution lifetimes, we observe that many layers individually exhibit homogeneous memory access characteristics as they internally embed homogeneous and dense computations. 2) We then profile external memory controller (EMC) utilization for all layers on both DLA and GPU. In Fig. 3, the input sizes of i1-i5 for the convolution layers correspond to (224,224,64), (224,112,64), (112,112,64), (112,56,64), (56,56,64) and filter sizes of f1-f5 correspond to (1x1), (2x2), (3x3), (4x4), (5x5), respectively. Our analysis reveals that the EMC utilization for DLA and GPU are correlated and proportional. 3) Using this observation, we estimate its memory throughput on black-box DSAs (e.g., DLA in this case) by dividing its GPU-based memory throughput by the ratio of EMC utilization of GPU and DSA for that specific layer. 4) Finally, by utilizing PCCS, we estimate the slowdown of a layer on an accelerator via its requested memory throughput and the external memory throughput requested by the other concurrently running layer on the other DSA.  246
Contention-aware Concurrent DNN Execution on Shared Memory SoCs PPoPP ’24, March 2–6, 2024, Edinburgh, United Kingdom  DSA1  t0 t1 t2 t3  L11  t4  DSA2  DSA3  L21  L31  L21  L22  Figure 4. Illustration for a hypothetical execution of five layers from three DNNs running on three different accelerators. Colored regions indicate additional slowdowns each layer experiences for varying external memory pressure.  When multiple layers are run simultaneously on different accelerators, the degree of slowdown throughout their execution is non-uniform and depends on the other layers running concurrently. Fig. 4 illustrates this behavior by depicting the execution timelines of five hypothetical layers belonging to three different DNNs. Each timeline represents the execution of ith layer on jth DNN, labeled as Li j on DSAk . The black regions in the timeline represent the time that the executions would take if all layers were run separately. Each colored extension to the black regions indicates slowdowns for different sets of layers running together. To address the complexity of handling varying amounts of slowdown during the execution of collocated layers, we introduce a scheduling concept called contention interval. Each contention interval (ti , ti+1) represents a period separated by the start or end of a layer execution, and it is represented by the Eq. 8 explained in Section 3.4. During each contention interval, different rates of slowdowns are observed by each layer, and the slowdown depends on the cumulative external memory pressure demands during that interval.  3.4 Formulating the problem  We integrate layer execution time, inter-accelerator transition time, and memory contention slowdown into a cost function and formulate the scheduling problem as a series of linear equations. Table 3 summarizes the variables and notations we use in the formulation. The primary input to our model is the DN N set for which we explore its mapping to the accelerator set A. Li,n denotes the smallest layer entity that belongs to the layer set of DN Nn. A layer entity is either a single layer or a group of layers, as explained in Section 3.1. Functions t (Li,n, a) and τ (Li,n, a, OUT |I N ) represent the execution time and transition overheads of layer Li,n on accelerator Aa, respectively. The goal of our formulation is to find the schedule S for all layers across all DNNs. The schedule function, defined in Eq. 1, returns Aa that Li,n should be mapped to. S is assumed to be initially unknown and will be determined by the solver later.  S (Li,n) = Aa where 1 ≤ i ≤ len(DN Nn),  1 ≤ n ≤ len(DN N ) , 1 ≤ a ≤ len(A) (1)  Total execution time of a DNN is formulated in Eq. 2. Total time comprises standalone execution time t of each layer, the slowdown C, and and I N and OUT transition costs, τ.  Table 3. The notation used by our formulation.  Notation Explanation  DN Nn nth DNN in the given DN N set which contains networks to be executed concurrently Li,n ith layer of the nth DNN in the DN N set len (DN Nn ) Total number (length) of layer groups in DN Nn Aa ath accelerator in the given accelerator set A S(Li,n) The schedule, i.e., accelerator mapping, of Li,n t (Li,n, Aa ) Total execution time of Li,n on Aa st (i, n) Execution start time of Li,n et (i, n) Execution end time of Li,n  τ (Li,n, Aa,  OUT |IN )  The time required to transition the DNN execution after|before layer li executed on accelerator Aa T Ri,n Boolean var if a transition is set after layer Li,n T (L, S (L) )n Total execution time elapsed by the execution of given sets of layer L of the nth DNN cLi,n,S (L),L The slowdown of Li,n due to the contention caused layers running on other accelerators, i.e., S (L) Ii,j The length of interval where layers i and j overlap Int Interval array holding start and end time of layers  T (L, S (L → A))n =  len (DN Nn ) ∑︁  i =0  t (Li,n, S (Li,n)) ∗ CLi,n,S (L),L  + T Ri,n × τ (Li, s (Li ), OUT ) + T Ri,n × τ (Li+1, s (Li+1), I N )  (2) We encode the decision to make transitions into our formulation via the boolean function given in Eq. 3. This function compares the accelerator assignments of adjacent layers Li,n and Li+1,n. If the assignments differ, the transition cost, τ, is subsequently incorporated into Eq. 2.  T Ri,n =  (  1 , if S (Li,n) ≠ S (Li+1,n)  0 , if S (Li,n) = S (Li+1,n) (3)  Eq. 4 and 5 compute the execution start and end times, st () and et () respectively, for layer Li,n. Int array in Eq. 6 stores the start and end time for layers, facilitating the iterative comparison of the contention intervals across layers.  st (i, n) = T (L0 to i−1 , n, S (L))n (4)  et (i, n) = st (i, n) + t (Li,n, S (Li,n)) ∗ Ci,n (5)  ∀Li,n , [st (i, n), et (i, n)] ∈ Int  where 1 ≤ i ≤ len(DN Nn), 1 ≤ n ≤ len(DN N ) (6)  The contention function C, outlined in Eq. 7, calculates the total slowdown for layer Li,n by taking each time overlapping with that layer and the slowdown ratio corresponding to the interval. The contention model returns an estimated slowdown amount depending on the bandwidth demanded by layer li and cumulative external bandwidth demanded by other layers running inside the same interval.  CLi,n,S (L),L =  ∑︁  Ik ∈Int  I (Li,n , Lj,n) ∗ cont _model (Li,n , Ls )  t (Li,n, S (Li,n)) ∗ len(Ls )  where 1 ≤ j ≤ len(DN Nn), 1 ≤ n ≤ len(DN N ), Lj,n ∈ Ls  Intk ∩ [sti,n, eti,n] ≠ ∅ , Intk ∩ [st j,n, et j,n] ≠ ∅  (7)  247
PPoPP ’24, March 2–6, 2024, Edinburgh, United Kingdom Ismet Dagli and Mehmet E. Belviranli  Eq. 8 details how we determine the duration of contention intervals. If a layer faces no contention, the equation simply returns the layer’s execution time, leading to a value of 1 to be returned in Eq. 7, thereby indicating no slowdown effect for a layer running independently in Eq. 2.  I (i, j) =        e j − si i f (s j ≤ si ≤ e j & si ≤ s j ≤ ei )  e j − s j i f (si ≤ s j ≤ ei & si ≤ s j ≤ ei )  ei − s j i f (si ≤ s j ≤ ei & s j ≤ ei ≤ e j )  ei − si i f (si ≤ s j & ei ≤ e j )  ei − si otherwise  (8)  We establish a constraint in Eq. 9 that limits two distinct layers from sharing the same accelerator for longer than an ε interval. Ideally, in a flawless model, the estimated execution and slowdown values could yield perfect transitions where accelerator usage periods can be precisely predicted. Variable ε allows us to mitigate the prediction errors, and facilitates more transition points by allowing for a tiny overlap of concurrently assigned layers on the same accelerator at the start or end of their executions.  Li,nn, Lj,n (Li,nn ∈ DN Nnn and Lj,n ∈ DN Nn |  stLj,n  < stLi,nn ∓ ε < etLi,nn  or stLj,n  < etLi,nn ∓ ε < etLj,n )  where S (Li,nn) = S (Lj,n) , nn ≠ n  (9)  Objective functions: Depending on the different scenarios that a user may target, we propose two separate objective functions: Equation 10 maximizes the utilization of the system to increase the total throughput and Equation 11 minimizes the maximum latency among DNNs. The use cases for objective functions are further elaborated in Section 5.  max  len(DN N ) ∑︁  n=1  1  T (L, S (L))n  (10) min max T (L, S (L))n (11)  3.5 Optimal and dynamic schedule generation  In our work, we target optimal schedules that satisfy given objectives and constraints because we don’t resort to heuristics to find such schedules. We achieve this by representing the entire scheduling problem formulated in Section 3.4 as a constraint-based optimization problem and solving with industry-strength SAT solvers such as Z3 [13], Gurobi [17], and OptiMathSAT [57]. These solvers employ branch & bound techniques to converge towards optimal solutions for many NP-complete problems (i.e., job-shop scheduling) [55]. Considering the relatively small parameter search space of our targeted problem set (i.e., total number of accelerators and tasks in the system), the use of SMT solvers provides optimal schedules in seconds. Depending on the operational requirements of the autonomous system, optimal schedules can be found either statically or dynamically. Generating optimal schedules beforehand (i.e., statically) is feasible for a variety of scenarios, such as in autonomous systems with fixed resolution input devices (like cameras  and lidars) and many DNNs designed for a fixed image or a video frame size. Some scenarios, such as a drone switching between discovery or tracking modes, might require unique control flow graphs (CFGs). Such CFGs (or the path followed in a CFG) and their corresponding schedules can be predetermined statically and toggled during the execution. Thus, users of HaX-CoNN can rely on offline profiling to determine the execution costs needed for static scheduling [72]. There are other cases where the static generation of optimal schedules may not be possible. For example, different DNN models may be required for various phases of the autonomous system execution [5, 21, 70], resulting in an unpredictable change in the CFG. For such scenarios, we propose D-HaX-CoNN, a runtime-based adaptation of our solution to (1) run SAT solvers on-the-fly, (2) gradually achieve and apply better schedules, and (3) eventually reach an optimal solution as the autonomous system continues to operate. This approach is feasible because autonomous systems often embed long-running loops and once an optimal schedule is found for a recently changed CFG, it will be reused for a while.  4 Experimental Setup  Computing platforms: We use three popular heterogeneous SoCs to evaluate HaX-CoNN : NVIDIA AGX Orin [47], Xavier AGX [46], and Qualcomm SnapDragon 865 development kit [53]. All three platforms have a shared memory with multiple accelerators. The technical specifications of these systems are summarized in Table 4. It is essential to note that the maximum number of accelerators we consider in our experiments is limited to two because, to the best of our knowledge, there are no off-the-shelf SoCs that offer more than two types of programmable DSAs for DNN acceleration.  Applications: We use the DNNs that are commonly used in benchmarking DNN inference: Alexnet [36], GoogleNet [61], Inception-V4 [59], ResNet18/52/101/152 [18], VGG-19  Table 4. The HW specifications for targeted architectures.  NVIDIA AGX Orin  GPU Ampere arch. 1792 CUDA & 64 Tensor cores DSA NVDLA v2.0 CPU 12-core Arm Cortex v8.2 64-bit Memory 32GB LPDDR5 | Bandwidth: 204.8 GB/s with 256-bit Software JetPack 5.0.1  NVIDIA Xavier AGX  GPU Volta arch. 512 CUDA and 64 Tensor cores DSA NVDLA v1.0 CPU 8-core Carmel Arm v8.2 64-bit Memory 16GB LPDDR4 | Bandwidth: 136.5 GB/s, 256-bit Software JetPack 4.5  Qualcomm 865 Mobile Development Kit GPU Qualcomm AdrenoTM 650 GPU DSA Hexagon 698 DSP CPU Qualcomm Kryo 585, 8-core, up to 2.84GHz Memory 6GB LPDDR5 | Bandwidth: 34.1 GB/s with 64 bits  248
Contention-aware Concurrent DNN Execution on Shared Memory SoCs PPoPP ’24, March 2–6, 2024, Edinburgh, United Kingdom  Table 5. Standalone runtimes (ms) and relative performance.  DNN  Device NVIDIA AGX Orin NVIDIA Xavier AGX  GPU (ms)  DLA (ms)  GPU (ms)  DLA (ms) CaffeNet 0.74 1.79 2.26 5.51 DenseNet 2.19 3.10 7.84 GoogleNet 0.99 1.52 1.98 3.68 Inc-res-v2 3.06 5.15 15.12 17.95 Inception 2.49 5.66 8.31 15.94 ResNet18 0.41 0.74 1.37 2.81 ResNet50 0.91 1.67 2.88 6.01 ResNet101 1.56 2.47 5.34 10.6 ResNet152 2.19 3.26 7.7 12.71 VGG19 1.07 2.93 5.95 19.05  [58], FCN-ResNet18, CaffeNet [26], DenseNet [22], and IncRes-v2 [60] with datasets from COCO [38], ImageNet ILSVRC [56], and Cityshape [9]. These DNNs could be used for various tasks in autonomous systems, such as object detection, image recognition, semantic segmentation, pose estimation, and depth estimation [15].  Profiling: Profiling duration varies by platforms: Computation, transition, and contention characterizations can take up to 3, 10, and 15 minutes per DNN model, respectively, on NVIDIA Orin, Xavier, and Qualcomm platforms whereas building engines require more time on NVIDIA boards. Since our approach is layer-centric, we performed profiling only once and it is offline.  Neural network synchronization: TensorRT natively does not provide support synchronization between the layers of DNNs concurrently running at different DSAs. To make sure that the inter-accelerator transitions across DNNs are properly performed, we implement a TensorRT plugin that employs inter-DNN synchronization via inter-process shared memory primitives.  Schedule generation: We solve our formulation given in Section 3.4 by using Z3 SMT solver. Z3 has shown superior performance for scheduling problems over popular solvers [55]. It works by determining the satisfiability of the constraints and finding an optimal solution for a given objective and constraints. In most of our experiments, Z3 takes under three seconds to run on a single CPU core of NVIDIA Orin AGX. In some cases, such as for the InceptionResNet-v2 network which consists of 985 layers, the solver takes around ten seconds to find the optimal schedule.  5 Evaluation  We demonstrate the utility of HaX-CoNN via four execution scenarios with different objectives and also via an experiment that exhaustively collocates all the DNNs in our evaluation set. Scenario 1 aims to maximize throughput in concurrent data processing on the same DNN whereas scenarios 2 and 3 target two different DNNs operating in parallel and in  a pipeline fashion, respectively. Scenario 4 is a hybrid of scenarios 2 and 3. We benchmark HaX-CoNN against five different baselines: (1) GPU only, (2) non-collaborative GPU & DLA, (3) Mensa [6] (which only supports single-DNN execution), (4) Herald [37], and (5) H2H [69] (which both support multi-DNN execution).  5.1 Running multiple instances of the same DNN  Scenario 1 - Concurrent image processing with same DNNs: In systems aiming for high throughput, multiple instances of the same DNN could concurrently process consecutive images. Fig. 5 reports the results of five different experiments designed for this scenario. The experiments are run on NVIDIA Orin and we compare HaX-CoNN against two naïve baselines and Mensa [6]. Overall, our experiments for this scenario show that HaX-CoNN can boost throughput (i.e., FPS) up to 29%. There are several key observations we make in this experiment: (1) In GoogleNet experiment, HaXCoNN maps the middle groups of layers (1-95 and 38-149) to GPU for both DNN instances since GPU executes those layers ∼2x faster than the DLA. (2) Due to shared memory contention, non-collaborative GPU & DLA execution does not always generate a better throughput compared to GPUonly execution. (3) We observe either limited improvements or no improvement by Mensa as it doesn’t consider shared memory contention, leading to mismatched layer transitions. Even though Mensa considers transition costs, its greedy strategy fails to account for the transition costs occurring in the future, leading to inaccurate transition decisions.  5.2 Concurrently running different type of DNNs  Table 6 lists the results of the experiments we performed by comparing HaX-CoNN to naïve and state-of-the-art multiDNN concurrent execution schemes. Experiments 1-5 are on Xavier AGX, 6-8 are on AGX Orin, and 9-10 are on Qualcomm 865. The second to fourth columns describe the experiment designs and the corresponding scenarios. There are four baselines we compare our work against: (1) GPU-only, (2) GPU & DSA, (3) Herald [37], (4) H2H [69]. The last three columns list the optimal schedules found by HaX-CoNN, the latency and throughput (i.e., FPS) for HaX-CoNN, and the improvement over the best-performing baseline.  Figure 5. Throughput (FPS) comparison for Scenario 1: Multiple instances of the same DNN is run concurrently on NVIDIA AGX Orin.  249
PPoPP ’24, March 2–6, 2024, Edinburgh, United Kingdom Ismet Dagli and Mehmet E. Belviranli  Table 6. Experiments run for Scenarios 2, 3, and 4. We compare these scenarios against baselines when run on NVIDIA Xavier AGX (in experiments 1-5), NVIDIA AGX Orin (in experiments 6-8), and Qualcomm 865 (in experiments 9-10). DSA refers to DLA for NVIDIA platforms and to the Hexagon DSP for the Qualcomm platform.  Exp  # Goal DNN-1 DNN-2  (1) GPU only  (2) GPU &  DSA (3) Herald (4) H2H  Optimal schedule by HaX-CoNN  Runtime of HaX-CoNN schedule  Improvement over the best baseline (%) Lat. FPS Lat. FPS Lat. FPS Lat. FPS TR Dir. Lat. FPS Lat. FPS  1 Min Latency  VGG-19 ResNet152 17.05 58 16.05 62 19.73 50 16.55 60 29  89  DtoG  GtoD 13.01 77 23 22  2 Min Latency  ResNet152 Inception 16.23 61 15.96 62 15.81 63 15.75 64 188  72  DtoG  GtoD 13.11 76 20 18  3 Max FPS  Alexnet ResNet101 11.04 90 10.97 93 12.10 82 11.49 87 11  161  GtoD  DtoG 8.7 115 26 23  4 Max FPS  ResNet101 GoogleNet 7.02 143 7.37 140 8.95 111 9.10 109 0  0  DtoG  DtoG 7.02 143 0 0  5 Min Latency  GoogleNet ResNet152  FC_ResN18 15.41 77 18.88 61 23.68 47 20.90 54 38  235  DtoG  GtoD 12.09 85 22 21  6 Min Latency  VGG-19 ResNet152 3.95 267 4.58 218 5.76 174 4.90 204 27  95  DtoG  GtoD 3.21 311 23 22  7 Max FPS  GoogleNet ResNet101 4.12 378 4.24 364 4.44 340 4.13 380 38  128  DtoG  GtoD 3.4 426 19 18  8 Min Latency  ResNet101 GoogleNet  Inception 5.06 197 4.97 201 5.56 180 4.91 203 31  88  DtoG  GtoD 4.41 226 13 12  9 Max FPS  GoogleNet ResNet101 98.3 10.1 79.1 12.6 95.9 10.4 113.8 8.8 52  148  DtoG  GtoD 71.08 14.1 11 10  10 Min Latency  Inception ResNet152 219.6 4.5 178.2 5.6 223.1 4.5 202.3 5.2 17  135  DtoG  GtoD 155.3 6.4 15 15  Scenario 2 - Two different DNNs operating on the same data: This scenario illustrates a case where different DNNs, such as object detection and image segmentation, process the same input in parallel, and they synchronize afterwards. The results are assumed to be passed on to subsequent tasks, such as motion planning [45], and then the loop is started over. Experiments 1, 2, 6, and 10 in Table 6 are run to demonstrate this scenario on three different target architectures. Our results show that HaX-CoNN improves both latency and throughput up to 23% in all four experiments of this scenario. We also observe that both H2H and Herald make inaccurate latency estimations that are wrong by up to 75% since neither of them considers shared memory contention. Experiments 1 and 6 show that HaX-CoNN results in different schedules for the same scenario running on different SoCs. For experiment 10, GPU & DSP is the best performing baseline for Qualcomm platform since GPU & DSP are more balanced on this platform in terms of their computation capability. Even though the schedule found by HaX-CoNN in experiment 10 on Qualcomm has a relatively higher transition cost among other transition candidates, the improvement primarily comes from minimizing the memory contention and effectively distributing the layers to DSAs.  Scenario 3 - Two different DNNs operating on streaming data: This scenario examines a common autonomous system setup where the input (e.g., camera stream) is available as a data stream and multiple tasks, such as object detection followed by object tracking [54], are executed in a  pipelined manner. This scenario is covered by experiments 3, 4, 7, and 9. To establish the dependency among DNNs, we connect the last layer of the DN N1 to the first layer of DN N2 as an input. Interestingly, HaX-CoNN opts not to use DLA for none of the layers in experiment 4 since running two images sequentially on the GPU yields a higher throughput. Particularly, the performance of DLA on ResNet18 is less than the slowdown imposed on the GPU. Overall, if there are cases where layer-level mapping does not foster any benefits, HaX-CoNN is capable of identifying these cases and utilizing the baseline solution instead. Our scheme guarantees that no worse results are obtained than the naïve baselines.  Scenario 4 - Multiple DNNs with concurrent and streaming data: In this scenario, two DNNs (DN N1 and DN N2) have a serial dependency in between and another DNN (DN N3) runs in parallel with the former two [54]. Experiments 5 and 8 belong to this scenario and the objective function is set to minimize the combined latency. HaX-CoNN is able to provide latency and throughput improvements up to 22%. Best performing baselines run DN N3 mostly on GPU since unbalanced workloads among accelerators and shared memory contention alleviate the advantages of concurrent utilization. In experiment 5, the schedules that use both DSAs concurrently perform worse than serialized GPU executions, since DLA is generally less effective in running fully-connected layers. In experiment 8, H2H provides the fastest baseline performance since they are capable of exploiting heterogeneity of DSAs for appropriate layers (e.g., such  250
Contention-aware Concurrent DNN Execution on Shared Memory SoCs PPoPP ’24, March 2–6, 2024, Edinburgh, United Kingdom
